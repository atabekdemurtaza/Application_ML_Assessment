{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnAUX7aF6eWd",
        "outputId": "fcd2392f-5601-4d19-822b-d35106086622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3162996 sha256=fc5c447813304fb79996bf49040ae0633bf2bd838f04cc71ecce211a1029fe39\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXPXG4nJ6hoC"
      },
      "outputs": [],
      "source": [
        "# Basic imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and data processing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from surprise import SVD, Dataset, Reader\n",
        "\n",
        "# Logging for debugging\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRLtfUQp69HV"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "def load_data(user_data_file, product_data_file):\n",
        "    try:\n",
        "        user_data = pd.read_csv(user_data_file, delimiter='\\t')\n",
        "        product_data = pd.read_csv(product_data_file, delimiter='\\t')\n",
        "\n",
        "        # Handle missing values and scale weights\n",
        "        user_data['rating'] = user_data['weight'] * 5 / user_data['weight'].max()\n",
        "        product_data.fillna(method='ffill', inplace=True)  # Forward filling missing values\n",
        "        logging.info(\"Data loaded and preprocessed successfully.\")\n",
        "        return user_data, product_data\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "croFfmFi6-sq"
      },
      "outputs": [],
      "source": [
        "# Collaborative filtering module\n",
        "def collaborative_filtering(user_data):\n",
        "    try:\n",
        "        reader = Reader(rating_scale=(1, 5))\n",
        "        data = Dataset.load_from_df(user_data[['userID', 'artistID', 'rating']], reader)\n",
        "        trainset = data.build_full_trainset()\n",
        "        algo = SVD()\n",
        "        algo.fit(trainset)\n",
        "        logging.info(\"Collaborative filtering model trained successfully.\")\n",
        "        return algo\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in collaborative filtering: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC0l5-L47J2i"
      },
      "outputs": [],
      "source": [
        "# Content-based filtering module with deep learning\n",
        "def content_based_filtering(product_data):\n",
        "    try:\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(product_data['name'])\n",
        "        sequences = tokenizer.texts_to_sequences(product_data['name'])\n",
        "        max_length = max([len(seq) for seq in sequences])\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "        # Deep learning model for product representation\n",
        "        embedding_dim = 128\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "            LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(product_data.shape[0], activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        model.fit(padded_sequences, np.eye(product_data.shape[0]), epochs=5, batch_size=32, validation_split=0.1)\n",
        "        product_representations = model.predict(padded_sequences)\n",
        "        product_similarity_matrix = cosine_similarity(product_representations)\n",
        "        logging.info(\"Content-based filtering model trained and similarity matrix created.\")\n",
        "        return product_similarity_matrix\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in content-based filtering: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLgWD2wG7Lo3"
      },
      "outputs": [],
      "source": [
        "# Visualization Functions\n",
        "def plot_histogram(recommendations, column='rating'):\n",
        "    if recommendations is not None and column in recommendations.columns:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(recommendations[column], bins=20, color='blue', alpha=0.7)\n",
        "        plt.title('Histogram of Ratings in Recommended Products')\n",
        "        plt.xlabel('Rating')\n",
        "        plt.ylabel('Number of Products')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid recommendations to display or specified column missing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tM9Aket7PjV"
      },
      "outputs": [],
      "source": [
        "def plot_similarity_matrix(matrix, labels=None):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, cmap='viridis')\n",
        "    plt.title('Product Similarity Matrix')\n",
        "    plt.xlabel('Products')\n",
        "    plt.ylabel('Products')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD2XeD0CIyCt"
      },
      "outputs": [],
      "source": [
        "# Hybrid recommendation system\n",
        "def hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model, alpha=0.5):\n",
        "    try:\n",
        "        user_interactions = user_data[user_data['userID'] == user_id]\n",
        "        collab_predictions = [collab_model.predict(user_id, product_id).est for product_id in product_data['id']]\n",
        "\n",
        "        user_liked_products = user_interactions[user_interactions['rating'] > 3]['artistID']\n",
        "        user_liked_products_indices = [product_data[product_data['id'] == product_id].index[0] for product_id in user_liked_products]\n",
        "\n",
        "        if user_liked_products_indices:\n",
        "            content_predictions = content_model[user_liked_products_indices].sum(axis=0) / len(user_liked_products_indices)\n",
        "        else:\n",
        "            content_predictions = np.zeros_like(collab_predictions)  # Default to zero or some other logic\n",
        "\n",
        "        hybrid_predictions = alpha * np.array(collab_predictions) + (1 - alpha) * np.array(content_predictions)\n",
        "        recommendations = product_data.iloc[np.argsort(hybrid_predictions)[::-1]]\n",
        "\n",
        "        return recommendations\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in hybrid recommendation: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkeTVr-bIyZ2"
      },
      "outputs": [],
      "source": [
        "# Main function to run the recommendation system\n",
        "def main():\n",
        "    user_data, product_data = load_data('user_artists.dat', 'artists.dat')\n",
        "    if user_data is not None and product_data is not None:\n",
        "        collab_model = collaborative_filtering(user_data)\n",
        "        content_model = content_based_filtering(product_data)\n",
        "        if collab_model is not None and content_model is not None:\n",
        "            user_id = 123\n",
        "            recommendations = hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model)\n",
        "            if recommendations is not None and 'rating' in recommendations.columns and not recommendations.empty:\n",
        "                print(recommendations)\n",
        "                plot_histogram(recommendations, 'rating')\n",
        "                plot_similarity_matrix(content_model)\n",
        "            else:\n",
        "                logging.info(\"No valid recommendations to display or 'rating' column missing.\")\n",
        "        else:\n",
        "            logging.error(\"Failed to train models or models returned None.\")\n",
        "    else:\n",
        "        logging.error(\"Data loading failed, cannot proceed with model training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUtt6w28Oqws",
        "outputId": "9811ae91-e415-41c9-96ed-cd9e598404ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "496/496 [==============================] - 38s 67ms/step - loss: 0.0314 - val_loss: 6.4926e-04\n",
            "Epoch 2/5\n",
            "496/496 [==============================] - 27s 54ms/step - loss: 6.8149e-04 - val_loss: 6.8107e-04\n",
            "Epoch 3/5\n",
            "496/496 [==============================] - 26s 53ms/step - loss: 6.8241e-04 - val_loss: 7.0059e-04\n",
            "Epoch 4/5\n",
            "496/496 [==============================] - 27s 54ms/step - loss: 6.8383e-04 - val_loss: 7.1551e-04\n",
            "Epoch 5/5\n",
            "496/496 [==============================] - 26s 52ms/step - loss: 6.8512e-04 - val_loss: 7.2961e-04\n",
            "551/551 [==============================] - 4s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ_Hht_tPqUK"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
        "logging.info(\"Data split into training and testing sets successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkWdcoOZAOjo",
        "outputId": "6b2c95d3-224f-462a-cee4-34415603761e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "551/551 [==============================] - 27s 37ms/step - loss: 0.0277\n",
            "Epoch 2/10\n",
            "551/551 [==============================] - 19s 35ms/step - loss: 6.8384e-04\n",
            "Epoch 3/10\n",
            "551/551 [==============================] - 18s 33ms/step - loss: 6.8776e-04\n",
            "Epoch 4/10\n",
            "551/551 [==============================] - 19s 35ms/step - loss: 6.9055e-04\n",
            "Epoch 5/10\n",
            "551/551 [==============================] - 18s 33ms/step - loss: 6.9153e-04\n",
            "Epoch 6/10\n",
            "551/551 [==============================] - 20s 36ms/step - loss: 6.9308e-04\n",
            "Epoch 7/10\n",
            "551/551 [==============================] - 19s 34ms/step - loss: 6.9287e-04\n",
            "Epoch 8/10\n",
            "551/551 [==============================] - 18s 33ms/step - loss: 6.9175e-04\n",
            "Epoch 9/10\n",
            "551/551 [==============================] - 19s 34ms/step - loss: 6.8931e-04\n",
            "Epoch 10/10\n",
            "551/551 [==============================] - 19s 35ms/step - loss: 6.8500e-04\n",
            "551/551 [==============================] - 4s 6ms/step\n",
            "          id                               name  \\\n",
            "5873    5998    Madonna feat. Justin Timberlake   \n",
            "5874    5999                             Zombie   \n",
            "5875    6000                 LanzamientosMp3.es   \n",
            "5876    6001                             Shamur   \n",
            "5877    6002                      Juliana Pasha   \n",
            "...      ...                                ...   \n",
            "11757  12183   Thao with The Get Down Stay Down   \n",
            "11758  12184                           トクマルシューゴ   \n",
            "11759  12185                           FREENOTE   \n",
            "11760  12186  Ella Fitzgerald & Louis Armstrong   \n",
            "11752  12178                    BEAT!BEAT!BEAT!   \n",
            "\n",
            "                                                     url  \\\n",
            "5873   http://www.last.fm/music/Madonna+feat.+Justin+...   \n",
            "5874                     http://www.last.fm/music/Zombie   \n",
            "5875         http://www.last.fm/music/LanzamientosMp3.es   \n",
            "5876                     http://www.last.fm/music/Shamur   \n",
            "5877              http://www.last.fm/music/Juliana+Pasha   \n",
            "...                                                  ...   \n",
            "11757  http://www.last.fm/music/Thao+with+The+Get+Dow...   \n",
            "11758  http://www.last.fm/music/%E3%83%88%E3%82%AF%E3...   \n",
            "11759                  http://www.last.fm/music/FREENOTE   \n",
            "11760  http://www.last.fm/music/Ella%2BFitzgerald%2B%...   \n",
            "11752     http://www.last.fm/music/BEAT%21BEAT%21BEAT%21   \n",
            "\n",
            "                                              pictureURL  \n",
            "5873   http://userserve-ak.last.fm/serve/252/30705889...  \n",
            "5874    http://userserve-ak.last.fm/serve/252/297390.jpg  \n",
            "5875   http://userserve-ak.last.fm/serve/252/24280293...  \n",
            "5876    http://userserve-ak.last.fm/serve/252/272812.jpg  \n",
            "5877   http://userserve-ak.last.fm/serve/252/47356893...  \n",
            "...                                                  ...  \n",
            "11757  http://userserve-ak.last.fm/serve/252/43814189...  \n",
            "11758  http://userserve-ak.last.fm/serve/252/12556183...  \n",
            "11759   http://userserve-ak.last.fm/serve/252/581912.png  \n",
            "11760    http://userserve-ak.last.fm/serve/252/20171.jpg  \n",
            "11752  http://userserve-ak.last.fm/serve/252/51885229...  \n",
            "\n",
            "[17632 rows x 4 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3c486bb385e4>:74: RuntimeWarning: invalid value encountered in divide\n",
            "  content_predictions = content_model[user_liked_products_indices].sum(axis=0) / len(user_liked_products_indices)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from surprise import SVD, Dataset, Reader\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data(user_data_file, product_data_file):\n",
        "    user_data = pd.read_csv(user_data_file, delimiter='\\t')\n",
        "    product_data = pd.read_csv(product_data_file, delimiter='\\t')\n",
        "\n",
        "    # Preprocess user data (e.g., handle missing values, encode categorical features)\n",
        "    user_data['rating'] = user_data['weight']*5/max(user_data['weight'])\n",
        "\n",
        "    # Preprocess product data (e.g., handle missing values, preprocess text data)\n",
        "    #product_data['rating'] = product_data['weight']*5/max(pruduct_data['weight'])\n",
        "\n",
        "    return user_data, product_data\n",
        "\n",
        "# Collaborative filtering module\n",
        "def collaborative_filtering(user_data):\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "    data = Dataset.load_from_df(user_data[['userID', 'artistID', 'rating']], reader)\n",
        "    trainset = data.build_full_trainset()\n",
        "    algo = SVD()\n",
        "    algo.fit(trainset)\n",
        "\n",
        "    return algo\n",
        "\n",
        "\n",
        "# Content-based filtering module with deep learning\n",
        "def content_based_filtering(product_data):\n",
        "    # Text preprocessing\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(product_data['name'])\n",
        "    sequences = tokenizer.texts_to_sequences(product_data['name'])\n",
        "    max_length = max([len(seq) for seq in sequences])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Deep learning model for product representation\n",
        "    embedding_dim = 128\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(product_data.shape[0], activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    model.fit(padded_sequences, np.eye(product_data.shape[0]), epochs=10, batch_size=32)\n",
        "\n",
        "    # Compute product similarity matrix\n",
        "    product_representations = model.predict(padded_sequences)\n",
        "    product_similarity_matrix = cosine_similarity(product_representations)\n",
        "\n",
        "    return product_similarity_matrix\n",
        "\n",
        "# Hybrid recommendation system\n",
        "def hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model, alpha=0.5):\n",
        "    user_interactions = user_data[user_data['userID'] == user_id]\n",
        "\n",
        "    # Collaborative filtering predictions\n",
        "    collab_predictions = [collab_model.predict(user_id, product_id)[3] for product_id in product_data['id']]\n",
        "\n",
        "    # Content-based filtering predictions\n",
        "    user_liked_products = user_interactions[user_interactions['rating'] > 3]['artistID']\n",
        "    user_liked_products_indices = [product_data[product_data['id'] == product_id].index[0] for product_id in user_liked_products]\n",
        "    content_predictions = content_model[user_liked_products_indices].sum(axis=0) / len(user_liked_products_indices)\n",
        "\n",
        "    # Hybrid recommendations\n",
        "    hybrid_predictions = alpha * np.array(collab_predictions) + (1 - alpha) * np.array(content_predictions)\n",
        "    hybrid_recommendations = product_data.iloc[hybrid_predictions.argsort()[::-1]]\n",
        "\n",
        "    return hybrid_recommendations\n",
        "\n",
        "# Example usage\n",
        "user_data, product_data = load_data('user_artists.dat', 'artists.dat')\n",
        "collab_model = collaborative_filtering(user_data)\n",
        "content_model = content_based_filtering(product_data)\n",
        "\n",
        "user_id = 123  # Example user ID\n",
        "hybrid_recommendations = hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model)\n",
        "print(hybrid_recommendations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgaOVoJ_Av6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}