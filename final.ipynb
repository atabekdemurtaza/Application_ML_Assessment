{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnAUX7aF6eWd",
        "outputId": "81984b38-8c31-44a4-eea0-541c1061637b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3162996 sha256=3e41487f0eac7ae1420280d27ec526cbeb377cf7ba03ac0665446c611fd046b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QXPXG4nJ6hoC"
      },
      "outputs": [],
      "source": [
        "# Basic imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and data processing\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from surprise import SVD, Dataset, Reader\n",
        "\n",
        "# Logging for debugging\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NRLtfUQp69HV"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "def load_data(user_data_file, product_data_file):\n",
        "    try:\n",
        "        user_data = pd.read_csv(user_data_file, delimiter='\\t')\n",
        "        product_data = pd.read_csv(product_data_file, delimiter='\\t')\n",
        "\n",
        "        # Handle missing values and scale weights\n",
        "        user_data['rating'] = user_data['weight'] * 5 / user_data['weight'].max()\n",
        "        product_data.fillna(method='ffill', inplace=True)  # Forward filling missing values\n",
        "        logging.info(\"Data loaded and preprocessed successfully.\")\n",
        "        return user_data, product_data\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "croFfmFi6-sq"
      },
      "outputs": [],
      "source": [
        "# Collaborative filtering module\n",
        "def collaborative_filtering(user_data):\n",
        "    try:\n",
        "        reader = Reader(rating_scale=(1, 5))\n",
        "        data = Dataset.load_from_df(user_data[['userID', 'artistID', 'rating']], reader)\n",
        "        trainset = data.build_full_trainset()\n",
        "        algo = SVD()\n",
        "        algo.fit(trainset)\n",
        "        logging.info(\"Collaborative filtering model trained successfully.\")\n",
        "        return algo\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in collaborative filtering: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZC0l5-L47J2i"
      },
      "outputs": [],
      "source": [
        "# Content-based filtering module with deep learning\n",
        "def content_based_filtering(product_data):\n",
        "    try:\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(product_data['name'])\n",
        "        sequences = tokenizer.texts_to_sequences(product_data['name'])\n",
        "        max_length = max([len(seq) for seq in sequences])\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "        # Deep learning model for product representation\n",
        "        embedding_dim = 128\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "            LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(product_data.shape[0], activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        model.fit(padded_sequences, np.eye(product_data.shape[0]), epochs=5, batch_size=32, validation_split=0.1)\n",
        "        product_representations = model.predict(padded_sequences)\n",
        "        product_similarity_matrix = cosine_similarity(product_representations)\n",
        "        logging.info(\"Content-based filtering model trained and similarity matrix created.\")\n",
        "        return product_similarity_matrix\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in content-based filtering: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rLgWD2wG7Lo3"
      },
      "outputs": [],
      "source": [
        "# Visualization Functions\n",
        "def plot_histogram(recommendations, column='rating'):\n",
        "    if recommendations is not None and column in recommendations.columns:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(recommendations[column], bins=20, color='blue', alpha=0.7)\n",
        "        plt.title('Histogram of Ratings in Recommended Products')\n",
        "        plt.xlabel('Rating')\n",
        "        plt.ylabel('Number of Products')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid recommendations to display or specified column missing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3tM9Aket7PjV"
      },
      "outputs": [],
      "source": [
        "def plot_similarity_matrix(matrix, labels=None):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, cmap='viridis')\n",
        "    plt.title('Product Similarity Matrix')\n",
        "    plt.xlabel('Products')\n",
        "    plt.ylabel('Products')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aD2XeD0CIyCt"
      },
      "outputs": [],
      "source": [
        "# Hybrid recommendation system\n",
        "def hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model, alpha=0.5):\n",
        "    try:\n",
        "        user_interactions = user_data[user_data['userID'] == user_id]\n",
        "        collab_predictions = [collab_model.predict(user_id, product_id).est for product_id in product_data['id']]\n",
        "\n",
        "        user_liked_products = user_interactions[user_interactions['rating'] > 3]['artistID']\n",
        "        user_liked_products_indices = [product_data[product_data['id'] == product_id].index[0] for product_id in user_liked_products]\n",
        "\n",
        "        if user_liked_products_indices:\n",
        "            content_predictions = content_model[user_liked_products_indices].sum(axis=0) / len(user_liked_products_indices)\n",
        "        else:\n",
        "            content_predictions = np.zeros_like(collab_predictions)  # Default to zero or some other logic\n",
        "\n",
        "        hybrid_predictions = alpha * np.array(collab_predictions) + (1 - alpha) * np.array(content_predictions)\n",
        "        recommendations = product_data.iloc[np.argsort(hybrid_predictions)[::-1]]\n",
        "\n",
        "        return recommendations\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in hybrid recommendation: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JkeTVr-bIyZ2"
      },
      "outputs": [],
      "source": [
        "# Main function to run the recommendation system\n",
        "def main():\n",
        "    user_data, product_data = load_data('user_artists.dat', 'artists.dat')\n",
        "    if user_data is not None and product_data is not None:\n",
        "        collab_model = collaborative_filtering(user_data)\n",
        "        content_model = content_based_filtering(product_data)\n",
        "        if collab_model is not None and content_model is not None:\n",
        "            user_id = 123\n",
        "            recommendations = hybrid_recommendation(user_id, user_data, product_data, collab_model, content_model)\n",
        "            if recommendations is not None and 'rating' in recommendations.columns and not recommendations.empty:\n",
        "                print(recommendations)\n",
        "                plot_histogram(recommendations, 'rating')\n",
        "                plot_similarity_matrix(content_model)\n",
        "            else:\n",
        "                logging.info(\"No valid recommendations to display or 'rating' column missing.\")\n",
        "        else:\n",
        "            logging.error(\"Failed to train models or models returned None.\")\n",
        "    else:\n",
        "        logging.error(\"Data loading failed, cannot proceed with model training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUtt6w28Oqws",
        "outputId": "9811ae91-e415-41c9-96ed-cd9e598404ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "496/496 [==============================] - 38s 67ms/step - loss: 0.0314 - val_loss: 6.4926e-04\n",
            "Epoch 2/5\n",
            "496/496 [==============================] - 27s 54ms/step - loss: 6.8149e-04 - val_loss: 6.8107e-04\n",
            "Epoch 3/5\n",
            "496/496 [==============================] - 26s 53ms/step - loss: 6.8241e-04 - val_loss: 7.0059e-04\n",
            "Epoch 4/5\n",
            "496/496 [==============================] - 27s 54ms/step - loss: 6.8383e-04 - val_loss: 7.1551e-04\n",
            "Epoch 5/5\n",
            "496/496 [==============================] - 26s 52ms/step - loss: 6.8512e-04 - val_loss: 7.2961e-04\n",
            "551/551 [==============================] - 4s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ_Hht_tPqUK"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
        "logging.info(\"Data split into training and testing sets successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
